---
title: "ElasticSearch and Redis streams"
date: 2018-01-09
categories: elastic redis
---

Redis Lists can be used as queues for jobs to move data from primary data store to ElasticSearch.  What if we have time-series data that needs to stay in Redis to be used by our application AND be copied to ElasticSearch?  

In previous [post]({% post_url 2018-01-04-elasticsearch-redis %}) we built a website for a nationwide retail chain.   We used Lists as queues but also had Redis SortedSets to record which zipcodes were searched and how often.  Separately we had counters (using Redis Strings) of when searches occurred by `day_of_week` and `hour_of_day`.  Our new requirement is to keep track of the exact time of each search and it's parameters (zipcode and product/query).  This will help our biz users to determine which products to stock in different stores.  

* TOC
{:toc}

### Redis Streams

This search data is usually recorded in our application logs as part of the GET request.  We could leverage [Logstash](https://www.elastic.co/products/logstash) to simply push data into ElasticSearch.  But we will now use [Streams](http://antirez.com/news/114) to record this data in Redis.  Streams are a new Redis data structure that is still in beta but should be released in 4.0.x timeframe.  

{% highlight ruby %}
class StoreLocator
  def initialize zipcode:, query:
    @zipcode = zipcode
    @query = query
    ...
  end
  def perform
    # search code here
    record_stats
  end
private
  def record_stats
    key = "search_log:#{Time.now.strftime("%Y-%m-%d")}"
    REDIS_CLIENT.xadd(key, '*', 'zip', @zip, 'query', @query)
  end
end
{% endhighlight %}

We are using the new `xadd` command which will create a new Redis stream and add an item to it with key/value pairs (zip and query).  Keys are based on timestamps to create one per day, just like we rotate logs.  Data in Redis will look like this:

{% highlight ruby %}
xrange search_log:2018-01-08 - +
1) 1) 1515465841276-0
   2) 1) "zip"
      2) "98168"
      3) "query"
      4) "Spilt Light"
2) 1) 1515465842278-0
   2) 1) "zip"
      2) "98114"
      3) "query"
      4) "Wake-up Volcano"
...
{% endhighlight %}

`xrange` is another new command that allows us to get the items from the stream.  `-` and `+` give us all items from first to last.  Stream IDs are autogenerated based on Unix epoch in milliseconds plus an a sequence number.  Alternatively we could have specified ID in `REDIS_CLIENT.xadd(key, numeric_id, ...)`.  

### ETL to ElasticSearch

Since the amount of data is quite large we decided to only keep the last 7 days of searches in Redis and move the older records to ElasticSearch.  Streams have flexible schema with different fields which fits well into ElasticSearch indexes.  

#### xrange

We will create corresponding ElasticSearch indexes such as "search_log:YYYY-MM-DD" and loop through stream items in batches.  We will also specify stream item ID as the ElasticSearch document ID.  

{% highlight ruby %}
ES_CLIENT = Elasticsearch::Client.new ...
# app/jobs/
class RedisElasticEtlJob < ApplicationJob
  def perform(num_days=7)
    key = "search_log:#{(Time.now - num_days.days).strftime("%Y-%m-%d")}"
    count = 100
    starting_id = '-'
    while true
      items = REDIS_CLIENT.xrange(key, starting_id, '+', 'count', count)
      items.each do |item|
        # => ["1515258610192-0", ["zip", "98134", "query", "Express Mug"]]
        hash = Hash[item.second.each_slice(2).to_a]
        # => {"zip": "98134", "query": "Express Mug"}
        hash['@timestamp'] = Time.strptime(item.first.to_i.to_s, '%Q')
        ES_CLIENT.index index: key, type: 'default', id: item.first, body: hash
      end
      break if items.count < count
      last_id = items.last.first.split('-')
      starting_id = [last_id.first, (last_id.second.to_i + 1).to_s].join('-')
    end
  end
end
{% endhighlight %}

Alternatively we can use `REDIS_CLIENT.xrevrange('search_log:YYYY-MM-DD', '+', '-'` to retrieve items in reverse order.  We can remove data from Redis either by setting TTL on different stream keys or manually doing `REDIS_CLIENT.del(''search_log:YYYY-MM-DD'')`.  

#### xread

What if we want more real-time data pipeline from Redis to ElasticSearch?  Instead of scheduled daily job we can build a dameon that will use the new `xread` command.  

{% highlight ruby %}
class RedisElasticStreamConsumer
  def perform
    while true
      key = "search_log:#{Time.now.strftime("%Y-%m-%d")}"
      data = REDIS.xread('BLOCK', 5000, 'STREAMS', key, '$')
      # => [["search_log:2018-01-07-21-35", [["1515389726944-0", ["zip", "98178", "query", "Red Select"]]]]]
      hash = Hash[data.first.second.first.second.each_slice(2).to_a]
      # => {"zip"=>"98178", "query"=>"Red Select"}
      id = data.first.second.first.first
      hash['@timestamp'] = Time.strptime(id.to_i.to_s, '%Q')
      ES_CLIENT.index index: key, type: 'default', id: id, body: hash
    end
  end
end
{% endhighlight %}

The challenge with this approach is that Redis will be able to record items in stream much faster than ElasticSearch can create documents (RAM vs disk).  While the scheduled job approach above is less cutting edge it may fit better into pre-existing ETL processes.  

### Using the data

To access data we can encapsulate the logic for choosing Redis vs ElasticSearch as our data source in separate class.  

{% highlight ruby %}

{% endhighlight %}

Separately we can use [ElasticSearch Kibana](https://www.elastic.co/products/kibana) to build interesting visualizations.  Logstash mentioned above has  input/output plugins for accessing data in Redis.  Currently they only support lists and channels but hopefully soon they will integrate with streams.  

### Other commands

#### xlen


#### maxlen

Capped streams

xadd stream_name numeric_id key value


{% highlight ruby %}

{% endhighlight %}




### Links
* https://brandur.org/redis-streams
* https://hackernoon.com/introduction-to-redis-streams-133f1c375cd3
* https://github.com/redis/redis-rcp/blob/master/RCP11.md
* https://gist.github.com/antirez/68e67f3251d10f026861be2d0fe0d2f4




{% highlight ruby %}

{% endhighlight %}



Use Redis as queue for jobs to update ElasticSearch.  Or application does LPUSH into Redis list and Logstash picks it up to move to ElasticSearch.  In previous post I gave example of using Logstash to LPUSH a message into Redis in ActiveJob format for processing via Sidekiq.  


Online game?  NFL leaderboard?  BART / Lightrail to track trips and charge $.  
